{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfa55fb-6ca9-4fec-920c-bfe6b67a5cf9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8ed7f8-7def-4eda-a741-45057faabb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b00014-1aa0-4685-913b-92b319cdd398",
   "metadata": {},
   "source": [
    "## Set var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245b6de4-d51c-4ac3-9529-7dcc1d8b7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_STRING = \">{} {}\\n{}\\n\"\n",
    "\n",
    "BASE_DIR = \"../../data/Combined_data\"\n",
    "pub = \"published\" # un-published is all\n",
    "\n",
    "path_COG = \"{}/{}/OGS\".format(BASE_DIR, pub)\n",
    "\n",
    "raw_fasta  = \"{}/{}/OGT_IMG_published_cd.fasta\".format(BASE_DIR, pub)\n",
    "\n",
    "test_fasta =  [\"COG0039.fasta\" , \"COG2032.fasta\"] # COGs that will be included in test set\n",
    "\n",
    "train_set = \"{}/{}/train.fasta\".format(BASE_DIR, pub)\n",
    "test_set = \"{}/{}/test.fasta\".format(BASE_DIR, pub)\n",
    "\n",
    "NAME_FORMAT = \"{}_{}_{}.fasta\"\n",
    "\n",
    "n_groups = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114f57a-950f-446c-a7ed-e34cb2273f71",
   "metadata": {},
   "source": [
    "# Split data in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58bfe9f-c9ee-4c4e-b375-3a544a27fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids_test = []\n",
    "\n",
    "\n",
    "# Write test set\n",
    "with open(test_set, \"w\") as file_writer:\n",
    "    for cog in test_fasta:\n",
    "        for rec in SeqIO.parse(os.path.join(path_COG, cog), \"fasta\"):\n",
    "            file_writer.write(FASTA_STRING.format(rec.id, rec.description.split()[-1], rec.seq))\n",
    "            ids_test.append(rec.id)\n",
    "            \n",
    "# Write train set\n",
    "with open(train_set, \"w\") as file_writer:\n",
    "    for rec in SeqIO.parse(raw_fasta,\"fasta\"):\n",
    "        if rec.id not in ids_test:\n",
    "            file_writer.write(FASTA_STRING.format(rec.id, rec.description.split()[-1], rec.seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea1ba8-9fd0-475f-a2b0-2119ea72bf05",
   "metadata": {},
   "source": [
    "# Make ranged sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6470a-877d-4c33-9bfb-edc27dd91edf",
   "metadata": {},
   "source": [
    "## fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b8e2290-c825-4c2a-9563-307ff62a4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data to get temp range\n",
    "\n",
    "data_train_all = {}\n",
    "temp_train_all = []\n",
    "for i, rec in enumerate(SeqIO.parse(train_set, \"fasta\")):\n",
    "    data_train_all[rec.id] = (rec.description.split()[-1], rec.seq)\n",
    "    temp_train_all.append(float(rec.description.split()[-1]))\n",
    "\n",
    "n_sequences_all = i\n",
    "temp_train_all = np.array(temp_train_all)\n",
    "min_temp = min(temp_train_all)\n",
    "max_temp = max(temp_train_all)\n",
    "\n",
    "ranges = np.linspace(min_temp, max_temp, n_groups)   \n",
    "up_sample = [n_sequences_all // (np.sum(temp_train_all < ranges[i+1]) - np.sum(temp_train_all < ranges[i])) for i in range(n_groups-1)]\n",
    "\n",
    "\n",
    "file_writers = []\n",
    "for i in range(n_groups-1):\n",
    "    temp_low = int(ranges[i])\n",
    "    temp_high = int(ranges[i+1])\n",
    "    upsample = int(up_sample[i])\n",
    "    name = os.path.join(BASE_DIR, pub, \"Groups_5/FASTA\", NAME_FORMAT.format(temp_low,temp_high,upsample))\n",
    "    file_writers.append(open(name, \"w\"))\n",
    "\n",
    "for i, rec in enumerate(SeqIO.parse(raw_fasta, \"fasta\")):\n",
    "    temp = float(rec.description.split()[-1])\n",
    "    idx = np.sum(ranges < temp+0.01)-1\n",
    "    if idx == n_groups-1:\n",
    "        idx = n_groups-2\n",
    "    file_writers[idx].write(FASTA_STRING.format(rec.id, temp, rec.seq))\n",
    "    \n",
    "\n",
    "for w in file_writers:\n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad56e2c-8cf3-422b-b749-4332ab6b941a",
   "metadata": {},
   "source": [
    "## tfrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f8cca-1f8e-4223-a645-d9c7b67d9c0f",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df5b4f0-5735-4695-8ad6-72e88bbfe64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "aas = 'XACDEFGHIKLMNPQRSTVWY'\n",
    "table = {aa: i-1 for i, aa in enumerate(aas)}\n",
    "table['U'] = -1\n",
    "def to_int(seq): \n",
    "    tmp = -np.ones((512,), dtype = np.int64)\n",
    "    seq = [ table[aa] for aa in str(rec.seq).upper()]\n",
    "    tmp[:len(seq)] = seq\n",
    "    return tmp\n",
    "\n",
    "def parse_ofset(name):\n",
    "    temp_low = int(name.split('_')[0])\n",
    "    temp_high= int(name.split('_')[1])\n",
    "    return temp_low + (temp_high - temp_low)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813e746-9e51-4431-9b19-eec333b3982f",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ccf08d8-c971-4a4e-b2dc-9e2c7370a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = os.path.join(BASE_DIR, pub, \"Groups_5/FASTA\")\n",
    "data_list = []\n",
    "for file in os.listdir(dir_):\n",
    "    if file[0] == '.':\n",
    "        continue\n",
    "    data = {\"id\": [], \"seq\": [], \"temp\": []}\n",
    "    for rec in SeqIO.parse(os.path.join(dir_, file), \"fasta\"):\n",
    "        data[\"id\"].append(rec.id)\n",
    "        data[\"seq\"].append(to_int( str(rec.seq).upper()))\n",
    "        data[\"temp\"].append(float(rec.description.split()[-1]))\n",
    "    data_list.append((data, file.split('.')[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71951b2d-d592-4628-92f0-781dc495cece",
   "metadata": {},
   "source": [
    "### Write records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63af71fc-3b52-41bc-9645-daa9f888f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data in data_list:    \n",
    "    name = os.path.join(BASE_DIR, pub, \"Groups_5/RECORDS\", data[1]+\".tfrecord\")\n",
    "    ofset = parse_ofset(data[1])\n",
    "    with tf.io.TFRecordWriter(name) as tfrecord:\n",
    "        for i in range(len(data[0][\"id\"])):\n",
    "            features = {\n",
    "              'temp': _float_feature(data[0][\"temp\"][i]-ofset),\n",
    "              'seq': _int64_feature(data[0][\"seq\"][i])\n",
    "              }\n",
    "            element = tf.train.Example(features = tf.train.Features(feature=features))\n",
    "            tfrecord.write(element.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f20a6-60ce-4355-87ff-4a2f49201dca",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03dbf2dd-ef5e-453d-89bb-0044f2a48376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(21,), dtype=float32)\n",
      "tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-5.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-2.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-2.5, shape=(), dtype=float32)\n",
      "tf.Tensor(5.5, shape=(), dtype=float32)\n",
      "tf.Tensor(5.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-7.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-7.5, shape=(), dtype=float32)\n",
      "tf.Tensor(-7.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../../utils/\")\n",
    "\n",
    "from preprocessing import load_data_class, load_data_reg\n",
    "\n",
    "config ={\n",
    "    \"base_dir\": os.path.join(BASE_DIR,pub,\"Groups_5\", \"RECORDS\"),\n",
    "    \"file_in\": \"82_103_105.tfrecord\",\n",
    "    \"file_out\": [\"22_42_1.tfrecord\", \"42_62_13.tfrecord\", \"62_82_34.tfrecord\", \"82_103_105.tfrecord\"],\n",
    "    \"chards\": 2\n",
    "}\n",
    "\n",
    "data_train, data_val = load_data_class(config)\n",
    "for d in data_train[0].skip(int(495)).take(10):\n",
    "    print(d[0][-1,:])\n",
    "\n",
    "data_train, data_val = load_data_reg(config)\n",
    "for d in data_val.take(10):\n",
    "    print(d[1])\n",
    "# Print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abdb4ecd-6e71-4b06-bc64-e0fde5d5df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.5\n",
      "12.0\n",
      "32.0\n",
      "52.0\n",
      "72.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e294ae-f70c-40cb-a00c-9b7f9fe9978f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/Combined_data/Groups_5/RECORDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-14f4ff75a93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/Combined_data/Groups_5/RECORDS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/Combined_data/Groups_5/RECORDS'"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../../data/Combined_data/published/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21d919-878d-473e-915b-979565051ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
